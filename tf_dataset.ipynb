{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tf_dataset.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP//GUkmpWsGTdU/Q0scQMG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adholmgren/tensorflow_playground/blob/master/tf_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIhbCtak8CYS",
        "colab_type": "text"
      },
      "source": [
        "Copyright 2020, Andrew Holmgren\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skZshN7Y85YR",
        "colab_type": "text"
      },
      "source": [
        "# A practical guide to the Dataset class in tensorflow\n",
        "The tensorflow Dataset class can streamline your machine learning application, but I've personally found many times when the documentation could benefit from some more examples or some more exposition. This notebook will hopefully serve as a guide, or at the very least a supplement, to the tensorflow documentation. An intermediate level of Python knowledge is assumed but not necessarily required, some concepts are best communicated with intermediate Python knowledge. Basic Python is required."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csllqN8Jpjar",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  !pip install --quiet tensorflow-gpu>=2.0.0"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6D8PWnkzlE7u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow.keras as K\n",
        "# tf.enable_eager_execution()"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cW6XGcP6nSWy",
        "colab_type": "text"
      },
      "source": [
        "# Build dataset from arrays"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sD-rO2aUGZwS",
        "colab_type": "text"
      },
      "source": [
        "If the data comes in as arrays it's pretty simple to get the data into a tensorflow dataset using the `from_tensor_slices` method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbAEiXD_lz12",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "arr = np.array([7, 2, 1, 6, 3, 5, 9])\n",
        "dataset = tf.data.Dataset.from_tensor_slices(arr)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPMpOJcg4V1a",
        "colab_type": "text"
      },
      "source": [
        "The tensor slices assumes the first dimension of the array is the dimension of new instances. For example, in the last code block it consumed the (7,) array and generated 7 instances of () shaped Tensors. As another example, the following code will consume 2 instances of 2x2 arrays."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8C1H8HnD4WWb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "3e2b51ae-f20a-4742-f428-c939d9a49a6e"
      },
      "source": [
        "dataset_2D = tf.data.Dataset.from_tensor_slices(np.reshape(np.arange(2**3), (2, 2, 2)))\n",
        "for elem in dataset_2D:\n",
        "    print(elem)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[0 1]\n",
            " [2 3]], shape=(2, 2), dtype=int64)\n",
            "tf.Tensor(\n",
            "[[4 5]\n",
            " [6 7]], shape=(2, 2), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8FUm9JnEAbp",
        "colab_type": "text"
      },
      "source": [
        "In comparison, this code will give just one instance of a 2x2x2 array."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0izNSFjD5d9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "16ac13fd-ddfc-4911-8804-f8517151ff84"
      },
      "source": [
        "dataset_3D = tf.data.Dataset.from_tensor_slices(np.reshape(np.arange(2**3), (1, 2, 2, 2)))\n",
        "for elem in dataset_3D:\n",
        "    print(elem)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[[0 1]\n",
            "  [2 3]]\n",
            "\n",
            " [[4 5]\n",
            "  [6 7]]], shape=(2, 2, 2), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jg8kpRN0EGSh",
        "colab_type": "text"
      },
      "source": [
        "**Test yourself**: build a dataset with 3 instances of a 3x3 array. Build 1 instance of a 3x10 array."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUl7LP1wQifP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 3 instances of 3x3 array\n",
        "# code here\n",
        "\n",
        "# 1 instance of a 3x10 array\n",
        "# code here"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dicUb1SQzP4",
        "colab_type": "text"
      },
      "source": [
        "Personally, I feel this next method is a bit redundant (I haven't looked into the source code, there could some slightly different optimization), but there's also a `from_tensor` method that creates a single instance. I think the context that makes the most sense for this method is testing at inference."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqIVSGELQyUX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "295d971c-9f27-431d-ac68-d5d75e182465"
      },
      "source": [
        "dataset_3D = tf.data.Dataset.from_tensors(np.reshape(np.arange(2**3), (2, 2, 2)))\n",
        "for elem in dataset_3D:\n",
        "    print(elem)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[[0 1]\n",
            "  [2 3]]\n",
            "\n",
            " [[4 5]\n",
            "  [6 7]]], shape=(2, 2, 2), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0LdGDFtUwaB",
        "colab_type": "text"
      },
      "source": [
        "You can see that this is the same thing as using  \n",
        "```Python\n",
        "dataset_3D = tf.data.Dataset.from_tensor_slices(np.reshape(np.arange(2**3), (1, 2, 2, 2)))\n",
        "```  \n",
        "Basically, if your array is a single instance this is a way for the Dataset class to consume the single instance. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPeOb7imIPUt",
        "colab_type": "text"
      },
      "source": [
        "# Using methods in the dataset class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqAg3cZPRfZ8",
        "colab_type": "text"
      },
      "source": [
        "### Dataset as an iterator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcISuxtRu19b",
        "colab_type": "text"
      },
      "source": [
        "Just like most Python objects, the dataset class has an iterator. In fact, the fundamental point of the dataset class is that its very essence is to be an iterator. All the methods within the dataset class either instantiate the iterator (e.g. `from_tensor_slices`), modify the iterator (e.g. `map`), or control the iterator (e.g. `batch`). The Dataset class gives users flexibility to control the memory and processing in flowing data inputs to Tensorflow's machine learning models. Most people can, and probably want to, stop there -- the dataset is fundamentally an iterator similar to Python's range, or numpy arrays, or a thousand other Python objects that iterate.\n",
        "\n",
        "For anyone interested in peaking a bit more into the nitty gritties read on. If you're not familiar with Python iterators and how they're built, here's a pretty good guide ([link](https://www.ics.uci.edu/~pattis/ICS-33/lectures/iterators.txt)). To really find out what's happening with the dataset iterator you have to go to the source code. The first thing that the source code lets you know is that it implements the Python iterator protocol and therefore can only be used in eager mode. This comment actually has the potential to cause confusion because the Dataset class can be used with a static graph, it will just function differently. An important consequence of eager vs. static execution, is that in static graph the entirety of arrays are placed into the static graph as Variables (potentially taking up a lot of memory and running into byte limits in graph serialization).\n",
        "```Python\n",
        "def __iter__(self):\n",
        "    \"\"\"Creates an `Iterator` for enumerating the elements of this dataset.\n",
        "    The returned iterator implements the Python iterator protocol and therefore\n",
        "    can only be used in eager mode.\n",
        "    Returns:\n",
        "      An `Iterator` over the elements of this dataset.\n",
        "    Raises:\n",
        "      RuntimeError: If not inside of tf.function and not executing eagerly.\n",
        "    \"\"\"\n",
        "    if (context.executing_eagerly()\n",
        "        or ops.get_default_graph()._building_function):  # pylint: disable=protected-access\n",
        "      return iterator_ops.OwnedIterator(self)\n",
        "    else:\n",
        "      raise RuntimeError(\"__iter__() is only supported inside of tf.function \"\n",
        "                         \"or when eager execution is enabled.\")\n",
        "```\n",
        "We then see that it uses the tensorflow iterator_ops for its iterator, so we can go to that source code. That source code points us to the [iterator_ops](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/data/ops/iterator_ops.py). Of which, the main elements are `_create_iterator` and `_next_internal`. These are effective the `__iter__` and `__next__` methods for the class. Those functions themselves have some more nitty gritties, but suffice to say that you can trace it far enough to reassure yourself that the iterator is moving through nests of the Tensor class. The gist of `_create_iterator` is that it has some pretty thorough robustness so you don't break it, but ultimately it is assigning pieces that fundamentally want to work with tensors. The `_next_internal` checks to see whether the backend is in eager_execution or static graph execution. In eager execution, where you'll most easily see the results, the next method is primarily concerned with returning tensor elements. There's a lot more tracking and integrating the core fundamentals than described thus far, but hopefully this at least provides a shallow insight into the class.\n",
        "```Python\n",
        "def _create_iterator(self, dataset):\n",
        "    # pylint: disable=protected-access\n",
        "    dataset = dataset._apply_options()\n",
        "\n",
        "    # Store dataset reference to ensure that dataset is alive when this iterator\n",
        "    # is being used. For example, `tf.data.Dataset.from_generator` registers\n",
        "    # a few py_funcs that are needed in `self._next_internal`.  If the dataset\n",
        "    # is deleted, this iterator crashes on `self.__next__(...)` call.\n",
        "    self._dataset = dataset\n",
        "\n",
        "    ds_variant = dataset._variant_tensor\n",
        "    self._element_spec = dataset.element_spec\n",
        "    self._flat_output_types = structure.get_flat_tensor_types(\n",
        "        self._element_spec)\n",
        "    self._flat_output_shapes = structure.get_flat_tensor_shapes(\n",
        "        self._element_spec)\n",
        "    with ops.colocate_with(ds_variant):\n",
        "      self._iterator_resource, self._deleter = (\n",
        "          gen_dataset_ops.anonymous_iterator_v2(\n",
        "              output_types=self._flat_output_types,\n",
        "              output_shapes=self._flat_output_shapes))\n",
        "      gen_dataset_ops.make_iterator(ds_variant, self._iterator_resource)\n",
        "      # Delete the resource when this object is deleted\n",
        "      self._resource_deleter = IteratorResourceDeleter(\n",
        "          handle=self._iterator_resource,\n",
        "          device=self._device,\n",
        "          deleter=self._deleter)\n",
        "\n",
        "def _next_internal(self):\n",
        "    \"\"\"Returns a nested structure of `tf.Tensor`s containing the next element.\n",
        "    \"\"\"\n",
        "    if not context.executing_eagerly():\n",
        "      with ops.device(self._device):\n",
        "        ret = gen_dataset_ops.iterator_get_next(\n",
        "            self._iterator_resource,\n",
        "            output_types=self._flat_output_types,\n",
        "            output_shapes=self._flat_output_shapes)\n",
        "      return structure.from_compatible_tensor_list(self._element_spec, ret)\n",
        "\n",
        "    # This runs in sync mode as iterators use an error status to communicate\n",
        "    # that there is no more data to iterate over.\n",
        "    # TODO(b/77291417): Fix\n",
        "    with context.execution_mode(context.SYNC):\n",
        "      with ops.device(self._device):\n",
        "        # TODO(ashankar): Consider removing this ops.device() contextmanager\n",
        "        # and instead mimic ops placement in graphs: Operations on resource\n",
        "        # handles execute on the same device as where the resource is placed.\n",
        "        ret = gen_dataset_ops.iterator_get_next(\n",
        "            self._iterator_resource,\n",
        "            output_types=self._flat_output_types,\n",
        "            output_shapes=self._flat_output_shapes)\n",
        "\n",
        "      try:\n",
        "        # Fast path for the case `self._structure` is not a nested structure.\n",
        "        return self._element_spec._from_compatible_tensor_list(ret)  # pylint: disable=protected-access\n",
        "      except AttributeError:\n",
        "        return structure.from_compatible_tensor_list(self._element_spec, ret)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oN_ZeO66Zz4F",
        "colab_type": "text"
      },
      "source": [
        "Here's the most common way Dataset iterator is used"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wH3Ju6JQ8i96",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create dataset\n",
        "arr = np.array([7, 2, 1, 6, 3, 5, 9])\n",
        "dataset = tf.data.Dataset.from_tensor_slices(arr)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X22Omo5ktP1E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "outputId": "6bb2d589-36fe-44fa-c75c-0350b38bf805"
      },
      "source": [
        "# loop through dataset elements\n",
        "for elem in dataset:\n",
        "    print(elem)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(7, shape=(), dtype=int64)\n",
            "tf.Tensor(2, shape=(), dtype=int64)\n",
            "tf.Tensor(1, shape=(), dtype=int64)\n",
            "tf.Tensor(6, shape=(), dtype=int64)\n",
            "tf.Tensor(3, shape=(), dtype=int64)\n",
            "tf.Tensor(5, shape=(), dtype=int64)\n",
            "tf.Tensor(9, shape=(), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-M2JnN9aET1",
        "colab_type": "text"
      },
      "source": [
        "If you only wanted a certain number of elements, you could even set conditional break in there."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpyJP4GLaS-s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "07c2529f-d45e-49b0-ec87-ac94fca490a6"
      },
      "source": [
        "count = 0\n",
        "n_elem = 3\n",
        "for elem in dataset:\n",
        "    print(elem)\n",
        "    count += 1\n",
        "    if count >= n_elem:\n",
        "        break"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(7, shape=(), dtype=int64)\n",
            "tf.Tensor(2, shape=(), dtype=int64)\n",
            "tf.Tensor(1, shape=(), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvXu5tSW-diS",
        "colab_type": "text"
      },
      "source": [
        "Another way to see just the first few elements is to work with the bare iterator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQ3-rVgg-ZQF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "a7c51cff-9b5f-4144-879b-741cb9701e5e"
      },
      "source": [
        "ds_iter = iter(dataset)\n",
        "print(f'first element: {next(ds_iter)}')\n",
        "print(f'second element: {next(ds_iter)}')\n",
        "print(f'third element: {next(ds_iter)}')\n",
        "del ds_iter"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "first element: 7\n",
            "second element: 2\n",
            "third element: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xDKViKc9aH5",
        "colab_type": "text"
      },
      "source": [
        "If you want to know what the for loop is really doing, we could do the functionally same operation written as below. The key is that the for loop goes until it hits a StopIteration exception that is built into the class's `next` method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwkwEnyU9Y_6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "outputId": "0c2f54db-912b-4958-85ec-eaad0b41cf4e"
      },
      "source": [
        "ds_iter = iter(dataset)\n",
        "try:\n",
        "    while True:\n",
        "        elem = next(ds_iter)\n",
        "        print(elem)\n",
        "except StopIteration:\n",
        "        pass\n",
        "finally:\n",
        "        del ds_iter"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(7, shape=(), dtype=int64)\n",
            "tf.Tensor(2, shape=(), dtype=int64)\n",
            "tf.Tensor(1, shape=(), dtype=int64)\n",
            "tf.Tensor(6, shape=(), dtype=int64)\n",
            "tf.Tensor(3, shape=(), dtype=int64)\n",
            "tf.Tensor(5, shape=(), dtype=int64)\n",
            "tf.Tensor(9, shape=(), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ll95DKJQZt8L",
        "colab_type": "text"
      },
      "source": [
        "Not as concise, is it?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRvsKP79D1vA",
        "colab_type": "text"
      },
      "source": [
        "## Take"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7jElLnusN4g",
        "colab_type": "text"
      },
      "source": [
        "Take, grab, give, this could have been called a lot of things but, alas, it is called `take`. This method takes a count (integer) and creates a new dataset that will iterate through tensors from at most count elements. The statement to reiterate is take builds a new dataset, it does not extract elements. Another tip worth noting is that you can overcount the take count and when the dataset iterates it will just go to max number of elements in the dataset. Similarly, any negative number simply takes the entire dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmh7HNljsMZA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "arr = np.array([7, 2, 1, 6, 3, 5, 9])\n",
        "dataset = tf.data.Dataset.from_tensor_slices(arr)"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nI1q6O0trqB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "ea3fdc73-ff7c-40f0-edd3-87332a9b38da"
      },
      "source": [
        "ds_take = dataset.take(3)\n",
        "print(list(ds_take.as_numpy_iterator()), \"\\n\")\n",
        "ds_take_more = dataset.take(1000)\n",
        "print(\"take over count: \", list(ds_take_more.as_numpy_iterator()))\n",
        "ds_take_neg = dataset.take(-8)\n",
        "print(\"take negative count (take all): \", list(ds_take_neg.as_numpy_iterator()))\n",
        "ds_take_none = dataset.take(0)\n",
        "print(\"take zero count (none): \", list(ds_take_none.as_numpy_iterator()))"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[7, 2, 1] \n",
            "\n",
            "take over count:  [7, 2, 1, 6, 3, 5, 9]\n",
            "take negative count (take all):  [7, 2, 1, 6, 3, 5, 9]\n",
            "take zero count (none):  []\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YL5qQ_UivWCv",
        "colab_type": "text"
      },
      "source": [
        "A decent analogy for `take` would be a wrapper on a generator with a stop condition that makes the first generator stop earlier if the count is reached and keep going if not.  Since the [true class](https://github.com/tensorflow/tensorflow/blob/e26286914b6d3cf7ed0c9a47f50c07391c2174a6/tensorflow/core/kernels/data/take_dataset_op.cc) is defined in C++, this analogy is merely illustrative."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COwMHtrQvUev",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Similar concept to Dataset.take\n",
        "def foo_generator():  # base iterator like Dataset\n",
        "    for i in range(10):\n",
        "        yield i \n",
        "\n",
        "def foo_take(count):\n",
        "    counter = 0\n",
        "    pity_the_foo = foo_generator()\n",
        "    run_all = count < 0\n",
        "    while True:\n",
        "        try:\n",
        "            if (counter >= count) and not run_all:\n",
        "                raise StopIteration\n",
        "            yield next(pity_the_foo)\n",
        "            counter += 1\n",
        "        except StopIteration:\n",
        "            break"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Karmjjc9wsyu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "e6cc36c3-4dd0-40ff-b85e-2750e3c007cf"
      },
      "source": [
        "print(list(foo_take(5)))\n",
        "print(list(foo_take(1000)))\n",
        "print(list(foo_take(-1)))\n",
        "print(list(foo_take(0)))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 1, 2, 3, 4]\n",
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "[]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFJMJMGzQMnu",
        "colab_type": "text"
      },
      "source": [
        "## Map dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNkftOO-8kyR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "arr = np.array([7, 2, 1, 6, 3, 5, 9])\n",
        "dataset = tf.data.Dataset.from_tensor_slices(arr)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g72VAcA9QWtV",
        "colab_type": "text"
      },
      "source": [
        "You can use map to apply a function to the dataset. For example, if you want to add 1 to to every element of the dataset you could do"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZ1iIrvVmCR6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "outputId": "7b449f2f-3222-4ff6-dd06-2fe391a94d70"
      },
      "source": [
        "def add_one(x):  # note: easily done anonymously lambda x: x + 1\n",
        "    return x + 1 \n",
        "\n",
        "dp1 = dataset.map(add_one)\n",
        "print(\"dataset \", \"mapped dataset\")\n",
        "for ds_elem, elem in zip(dataset, dp1):\n",
        "    print(ds_elem.numpy(), \"\\t\", elem.numpy())"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dataset  mapped dataset\n",
            "7 \t 8\n",
            "2 \t 3\n",
            "1 \t 2\n",
            "6 \t 7\n",
            "3 \t 4\n",
            "5 \t 6\n",
            "9 \t 10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyV1gQn_Qng0",
        "colab_type": "text"
      },
      "source": [
        "As another example of a mapping function, we can make a function that one-hot encodes values corresponding to a class index."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MR0jPfHLszlV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "outputId": "7cd79766-2d87-4d3f-ab81-d2b4fccbce08"
      },
      "source": [
        "def one_hot_encode(x, max_index):\n",
        "    tensor_zeros = tf.zeros(max_index, dtype=tf.int32)\n",
        "    tensor_ones = tf.ones(max_index, dtype=tf.int32)\n",
        "    tensor_range = tf.range(max_index, dtype=tf.int32)\n",
        "    # dataset defaulted to int64 type, hence the casting, types may vary by context\n",
        "    one_hot_array = tf.where(tensor_range == tf.cast(x, tf.int32), tensor_ones, tensor_zeros)\n",
        "    return one_hot_array\n",
        "\n",
        "ds_one_hot = dataset.map(lambda x: one_hot_encode(x, tf.constant(10, dtype=tf.int32)))\n",
        "for elem in ds_one_hot:\n",
        "    print(elem)"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([0 0 0 0 0 0 0 1 0 0], shape=(10,), dtype=int32)\n",
            "tf.Tensor([0 0 1 0 0 0 0 0 0 0], shape=(10,), dtype=int32)\n",
            "tf.Tensor([0 1 0 0 0 0 0 0 0 0], shape=(10,), dtype=int32)\n",
            "tf.Tensor([0 0 0 0 0 0 1 0 0 0], shape=(10,), dtype=int32)\n",
            "tf.Tensor([0 0 0 1 0 0 0 0 0 0], shape=(10,), dtype=int32)\n",
            "tf.Tensor([0 0 0 0 0 1 0 0 0 0], shape=(10,), dtype=int32)\n",
            "tf.Tensor([0 0 0 0 0 0 0 0 0 1], shape=(10,), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWjBfpr4pW88",
        "colab_type": "text"
      },
      "source": [
        "Alternatively, rather than manually making a one hot encoding function you could just use tensorflows built-in one_hot function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHXuwuGCjcyh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "outputId": "bc43e8ee-5469-4f61-d047-c5ba628e719e"
      },
      "source": [
        "ds_one_hot = dataset.map(lambda x: tf.one_hot(x, tf.constant(10, dtype=tf.int32), dtype=tf.int32))\n",
        "for elem, one_hot_elem in zip(dataset, ds_one_hot):\n",
        "    print(f'category index {elem.numpy()}, one_hot_encoding {one_hot_elem.numpy()}')"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "category index 7, one_hot_encoding [0 0 0 0 0 0 0 1 0 0]\n",
            "category index 2, one_hot_encoding [0 0 1 0 0 0 0 0 0 0]\n",
            "category index 1, one_hot_encoding [0 1 0 0 0 0 0 0 0 0]\n",
            "category index 6, one_hot_encoding [0 0 0 0 0 0 1 0 0 0]\n",
            "category index 3, one_hot_encoding [0 0 0 1 0 0 0 0 0 0]\n",
            "category index 5, one_hot_encoding [0 0 0 0 0 1 0 0 0 0]\n",
            "category index 9, one_hot_encoding [0 0 0 0 0 0 0 0 0 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_Wyy0xdAUgN",
        "colab_type": "text"
      },
      "source": [
        "To really do the one-hot encoding to death, note that the tensors could also return as a sparse type."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnZLPDlNAJui",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        },
        "outputId": "40a7fe62-da13-4409-e411-596065a95e67"
      },
      "source": [
        "def one_hot_encode(x, max_index):\n",
        "    return tf.sparse.SparseTensor(indices=[[x]], values=[tf.cast(1, dtype=tf.int64)], dense_shape=[max_index])\n",
        "\n",
        "ds_one_hot = dataset.map(lambda x: one_hot_encode(x, tf.constant(10, dtype=tf.int64)))\n",
        "for elem in ds_one_hot:\n",
        "    print(elem)\n",
        "    print(tf.sparse.to_dense(elem))"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SparseTensor(indices=tf.Tensor([[7]], shape=(1, 1), dtype=int64), values=tf.Tensor([1], shape=(1,), dtype=int64), dense_shape=tf.Tensor([10], shape=(1,), dtype=int64))\n",
            "tf.Tensor([0 0 0 0 0 0 0 1 0 0], shape=(10,), dtype=int64)\n",
            "SparseTensor(indices=tf.Tensor([[2]], shape=(1, 1), dtype=int64), values=tf.Tensor([1], shape=(1,), dtype=int64), dense_shape=tf.Tensor([10], shape=(1,), dtype=int64))\n",
            "tf.Tensor([0 0 1 0 0 0 0 0 0 0], shape=(10,), dtype=int64)\n",
            "SparseTensor(indices=tf.Tensor([[1]], shape=(1, 1), dtype=int64), values=tf.Tensor([1], shape=(1,), dtype=int64), dense_shape=tf.Tensor([10], shape=(1,), dtype=int64))\n",
            "tf.Tensor([0 1 0 0 0 0 0 0 0 0], shape=(10,), dtype=int64)\n",
            "SparseTensor(indices=tf.Tensor([[6]], shape=(1, 1), dtype=int64), values=tf.Tensor([1], shape=(1,), dtype=int64), dense_shape=tf.Tensor([10], shape=(1,), dtype=int64))\n",
            "tf.Tensor([0 0 0 0 0 0 1 0 0 0], shape=(10,), dtype=int64)\n",
            "SparseTensor(indices=tf.Tensor([[3]], shape=(1, 1), dtype=int64), values=tf.Tensor([1], shape=(1,), dtype=int64), dense_shape=tf.Tensor([10], shape=(1,), dtype=int64))\n",
            "tf.Tensor([0 0 0 1 0 0 0 0 0 0], shape=(10,), dtype=int64)\n",
            "SparseTensor(indices=tf.Tensor([[5]], shape=(1, 1), dtype=int64), values=tf.Tensor([1], shape=(1,), dtype=int64), dense_shape=tf.Tensor([10], shape=(1,), dtype=int64))\n",
            "tf.Tensor([0 0 0 0 0 1 0 0 0 0], shape=(10,), dtype=int64)\n",
            "SparseTensor(indices=tf.Tensor([[9]], shape=(1, 1), dtype=int64), values=tf.Tensor([1], shape=(1,), dtype=int64), dense_shape=tf.Tensor([10], shape=(1,), dtype=int64))\n",
            "tf.Tensor([0 0 0 0 0 0 0 0 0 1], shape=(10,), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tDCdbYpDBVV",
        "colab_type": "text"
      },
      "source": [
        "## Filter dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQULfD0eIKwy",
        "colab_type": "text"
      },
      "source": [
        "Both map and filter applies a function to each element in a dataset. Wheras map uses a function to alter the dataset elements in some way, filter works on a conditional return to delete dataset elements that fail the conditional test. The filter method is good for deleting bad dataset inputs.  \n",
        "\n",
        "Let's imagine a dataset of images that vary in length and width. Most pretrained networks are trained for images of sizes 299x299, 256x256, or 224x224. So if images are coming in the general size of 300x300, some more (say 350x350) and some less (say 250x250) then for the most part I can safely crop down to the right size. However, if the data augmentation involves rotation or other affine transforms then extra support around the input size will be desired, and as such some images may need to get thrown out. The following example goes through such a case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fg7gTTw7M_cZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# generate random array of random size\n",
        "def gen_series(n_samps):\n",
        "  i = 0\n",
        "  while True:\n",
        "    rand_shape = np.random.randint(2, 5)\n",
        "    # note need to explicitly output shape because a symbolic tensor does not have a shape \n",
        "    yield (rand_shape, np.random.random(size=(rand_shape, rand_shape)))\n",
        "    i += 1\n",
        "    if i > n_samps:\n",
        "        break\n"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faqKiLJW83q5",
        "colab_type": "text"
      },
      "source": [
        "Now set the random seed for reproducibility and make 5 random numpy arrays."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMjtD0N1JvIZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "outputId": "0aee30b7-d45a-4315-a663-220bebe9c2be"
      },
      "source": [
        "np.random.seed(314)\n",
        "for elem in gen_series(5):\n",
        "    print(elem)"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2, array([[0.97120896, 0.481791  ],\n",
            "       [0.9738772 , 0.59946984]]))\n",
            "(4, array([[0.82735501, 0.72795148, 0.26048042, 0.9117634 ],\n",
            "       [0.26075656, 0.76637602, 0.26153114, 0.12229137],\n",
            "       [0.38600554, 0.84008124, 0.27817936, 0.06991369],\n",
            "       [0.63310965, 0.58476603, 0.58123194, 0.6772054 ]]))\n",
            "(4, array([[0.39143885, 0.16435973, 0.43433933, 0.74557941],\n",
            "       [0.97003736, 0.35446608, 0.49190316, 0.30551103],\n",
            "       [0.44273468, 0.38317651, 0.57375445, 0.5094681 ],\n",
            "       [0.32474148, 0.46083002, 0.00804761, 0.45918614]]))\n",
            "(4, array([[0.40695651, 0.17784994, 0.90925204, 0.545331  ],\n",
            "       [0.1004968 , 0.71872059, 0.97842935, 0.3097757 ],\n",
            "       [0.26012577, 0.66289961, 0.13971997, 0.08372171],\n",
            "       [0.52679728, 0.6102353 , 0.86738912, 0.14893502]]))\n",
            "(3, array([[0.27418571, 0.40196772, 0.16730927],\n",
            "       [0.45452528, 0.84794886, 0.45265904],\n",
            "       [0.85438408, 0.04457095, 0.43005191]]))\n",
            "(3, array([[0.98196678, 0.04156222, 0.09930461],\n",
            "       [0.22048275, 0.66002216, 0.99614822],\n",
            "       [0.96977814, 0.67655364, 0.99848073]]))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mlISeCvKvAR",
        "colab_type": "text"
      },
      "source": [
        "Turn the arrays into a dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itYcFMCtKuGT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(314)\n",
        "dataset_shapes = tf.data.Dataset.from_generator(gen_series,\n",
        "                                                output_types=(tf.int32, tf.float32),\n",
        "                                                args = [5]\n",
        "                                                )"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nA-sGx0o9Dsm",
        "colab_type": "text"
      },
      "source": [
        "Filter out any arrays that have a shape less than 3 (or conversely keep arrays larger than 2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6E99bIQgg7K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 344
        },
        "outputId": "ffb75228-05c9-4c11-c052-47391df9337a"
      },
      "source": [
        "def filter_shape(shape, arr):\n",
        "    return shape > 2\n",
        "\n",
        "np.random.seed(314)\n",
        "ds_filter = dataset_shapes.filter(filter_shape)\n",
        "list(ds_filter.as_numpy_iterator())"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(4, array([[0.827355  , 0.72795147, 0.26048043, 0.91176337],\n",
              "         [0.26075655, 0.766376  , 0.26153114, 0.12229137],\n",
              "         [0.38600555, 0.8400812 , 0.27817935, 0.06991369],\n",
              "         [0.6331096 , 0.58476603, 0.58123195, 0.6772054 ]], dtype=float32)),\n",
              " (4, array([[0.39143884, 0.16435973, 0.4343393 , 0.7455794 ],\n",
              "         [0.97003734, 0.35446608, 0.49190316, 0.30551103],\n",
              "         [0.4427347 , 0.3831765 , 0.57375443, 0.5094681 ],\n",
              "         [0.32474148, 0.46083003, 0.00804761, 0.45918614]], dtype=float32)),\n",
              " (4, array([[0.40695652, 0.17784993, 0.90925205, 0.545331  ],\n",
              "         [0.1004968 , 0.7187206 , 0.9784294 , 0.3097757 ],\n",
              "         [0.2601258 , 0.6628996 , 0.13971996, 0.08372171],\n",
              "         [0.5267973 , 0.6102353 , 0.86738914, 0.14893502]], dtype=float32)),\n",
              " (3, array([[0.27418572, 0.40196773, 0.16730927],\n",
              "         [0.4545253 , 0.84794885, 0.45265904],\n",
              "         [0.85438406, 0.04457095, 0.43005192]], dtype=float32)),\n",
              " (3, array([[0.9819668 , 0.04156223, 0.09930461],\n",
              "         [0.22048275, 0.66002214, 0.9961482 ],\n",
              "         [0.9697781 , 0.67655367, 0.99848074]], dtype=float32))]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAtrDF3c0PUm",
        "colab_type": "text"
      },
      "source": [
        "Another case where a filter could be helpful is if the data hits some value then you know it's corrupt (maybe a CRC check, or just physical constraints). For example, it could be that you know its impossible for a speed-o-meter to register a speed less than 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHLrrOxp0oiu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "outputId": "bbfd26f5-b6a3-434a-b183-9110ce790c20"
      },
      "source": [
        "np.random.seed(272)\n",
        "dataset_threshold = tf.data.Dataset.from_tensor_slices(np.random.rand(10)).map(lambda x: x - 0.3)\n",
        "list(dataset_threshold.as_numpy_iterator())"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[-0.05227946474140316,\n",
              " 0.5785707195716125,\n",
              " 0.6378979597461731,\n",
              " 0.006009622292020567,\n",
              " 0.16534980848240438,\n",
              " -0.12634543430508632,\n",
              " -0.0776061455831683,\n",
              " -0.017021838344000118,\n",
              " 0.317895438599268,\n",
              " -0.1027123945757819]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEvqimxT-wsm",
        "colab_type": "text"
      },
      "source": [
        "Keep any values that are above 0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGm6FzAC-hMV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "f21b0266-df51-4806-bd07-d009faab57e9"
      },
      "source": [
        "list(dataset_threshold.filter(lambda x: x > 0).as_numpy_iterator())"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.5785707195716125,\n",
              " 0.6378979597461731,\n",
              " 0.006009622292020567,\n",
              " 0.16534980848240438,\n",
              " 0.317895438599268]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oo6shlT1-9k_",
        "colab_type": "text"
      },
      "source": [
        "Try it out for yourself. Make an array with normally distributed data and filter out an events more than 2*sigma."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CiKDE7gWolw",
        "colab_type": "text"
      },
      "source": [
        "## Interleave"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIpjB0EPW7TJ",
        "colab_type": "text"
      },
      "source": [
        "Building from the concepts of `map`, `interleave` is `map` operation. What is different, though, is that `interleave` is a map of a dataset. Wheras `map` operates on tensor elements and typically involves processing those elements, the `interleave` method operates on a dataset itself and typically involves creating more samples. In this sense interleave often builds on a map."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQIfQGLSa07g",
        "colab_type": "text"
      },
      "source": [
        "Personally, I found this function was really difficult to wrap my head around.\n",
        "\n",
        "Interleave has two main controls, `cycle_length` and `block_length`.   \n",
        "  * `cycle_length` \n",
        "  * `block_length`\n",
        "\n",
        "Cycle length is how many times to cycle through the datasets before starting to interleave, and block length is the length of the interleave."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mywyost_LyDZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 562
        },
        "outputId": "a6bf980f-642d-41d6-db7c-ab23772ea28d"
      },
      "source": [
        "dataset = tf.data.Dataset.range(1, 6)  # ==> [ 1, 2, 3, 4, 5 ]\n",
        "# NOTE: New lines indicate \"block\" boundaries.\n",
        "dataset = dataset.interleave(\n",
        "    lambda x: tf.data.Dataset.from_tensors(x).repeat(6),\n",
        "    cycle_length=3, block_length=4)\n",
        "list(dataset.as_numpy_iterator())\n"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 3,\n",
              " 3,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 4,\n",
              " 4,\n",
              " 5,\n",
              " 5]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lA9skUBgXPHT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_arr = [['a'], ['b']]\n",
        "num_array = [np.arange(4), np.arange(6, 10)]"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNRWuqeIez2m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f38e1d6c-9dab-43b6-9b7a-b531b087a9b8"
      },
      "source": [
        "transform = lambda x: -x\n",
        "dataset = tf.data.Dataset.from_tensor_slices(base_arr)\n",
        "print(list(dataset.as_numpy_iterator()))"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[array([b'a'], dtype=object), array([b'b'], dtype=object)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWTJogF5Y8QQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 794
        },
        "outputId": "517c7f06-51e8-4eb3-b31e-41a66ae0a635"
      },
      "source": [
        "interleaved_set = dataset.interleave(lambda x: tf.data.Dataset.from_tensor_slices(num_array),\n",
        "                                     cycle_length=1,\n",
        "                                     block_length=1,\n",
        "                                     )\n",
        "list(interleaved_set.as_numpy_iterator())"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-79-913bc261822a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m interleaved_set = dataset.interleave(lambda x: tf.data.Dataset.from_tensor_slices(num_array),\n\u001b[1;32m      2\u001b[0m                                      \u001b[0mcycle_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m                                      \u001b[0mblock_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m                                      )\n\u001b[1;32m      5\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minterleaved_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_numpy_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36minterleave\u001b[0;34m(self, map_func, cycle_length, block_length, num_parallel_calls, deterministic)\u001b[0m\n\u001b[1;32m   1829\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1830\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1831\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mInterleaveDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1832\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1833\u001b[0m       return ParallelInterleaveDataset(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func, cycle_length, block_length)\u001b[0m\n\u001b[1;32m   4153\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4154\u001b[0m     self._map_func = StructuredFunctionWrapper(\n\u001b[0;32m-> 4155\u001b[0;31m         map_func, self._transformation_name(), dataset=input_dataset)\n\u001b[0m\u001b[1;32m   4156\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_func\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDatasetSpec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4157\u001b[0m       raise TypeError(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m   3369\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource_tracker_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_tracker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3370\u001b[0m         \u001b[0;31m# TODO(b/141462134): Switch to using garbage collection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3371\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_concrete_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3372\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0madd_to_graph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3373\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2937\u001b[0m     \"\"\"\n\u001b[1;32m   2938\u001b[0m     graph_function = self._get_concrete_function_garbage_collected(\n\u001b[0;32m-> 2939\u001b[0;31m         *args, **kwargs)\n\u001b[0m\u001b[1;32m   2940\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_garbage_collector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2941\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2904\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2905\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2906\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2907\u001b[0m       \u001b[0mseen_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2908\u001b[0m       captured = object_identity.ObjectIdentitySet(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3213\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3214\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3215\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3073\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3074\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3075\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3076\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3077\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mwrapper_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   3362\u001b[0m           attributes=defun_kwargs)\n\u001b[1;32m   3363\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=missing-docstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3364\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_wrapper_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3365\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3366\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m_wrapper_helper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   3297\u001b[0m         \u001b[0mnested_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3299\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3300\u001b[0m       \u001b[0;31m# If `func` returns a list of tensors, `nest.flatten()` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3301\u001b[0m       \u001b[0;31m# `ops.convert_to_tensor()` would conspire to attempt to stack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    256\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m           \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    <ipython-input-76-913bc261822a>:1 None  *\n        interleaved_set = dataset.interleave(lambda x: tf.data.Dataset.from_tensor_slices(num_array),\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py:682 from_tensor_slices  **\n        return TensorSliceDataset(tensors)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py:3001 __init__\n        element = structure.normalize_element(element)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/util/structure.py:115 normalize_element\n        ops.convert_to_tensor(t, name=\"component_%d\" % i))\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:1499 convert_to_tensor\n        ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py:338 _constant_tensor_conversion_function\n        return constant(v, dtype=dtype, name=name)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py:264 constant\n        allow_broadcast=True)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py:282 _constant_impl\n        allow_broadcast=allow_broadcast))\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:465 make_tensor_proto\n        _GetDenseDimensions(values)))\n\n    ValueError: Argument must be a dense tensor: [[array([0, 1, 2, 3])], [array([6, 7, 8, 9])]] - got shape [2, 1, 4], but wanted [2, 1].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkPLZBDlW7zQ",
        "colab_type": "text"
      },
      "source": [
        "## Shuffle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chl095YwxMJ9",
        "colab_type": "text"
      },
      "source": [
        "Shuffle is pretty much what you imagine, it takes your dataset and will spit out a random permutation of your dataset. The main nuance to the shuffle function is the buffer size argument. That being said, the buffer size argument is itself pretty nuanced."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7EaryvYzvYG",
        "colab_type": "text"
      },
      "source": [
        "Take the following example: there's a dataset with numbers one through 9.  \n",
        "With a buffer size of 2, the first shuffle picks from  \n",
        "[0, 1]  \n",
        "let's say that it picks 1, now the buffer has  \n",
        "[0, 2]  \n",
        "let's say that it picks 2 this time around, now the buffer has  \n",
        "[0, 3]  \n",
        "and the shuffled array, to date, has  \n",
        "[1, 2, 3]  \n",
        "elements. As such, the placement of 0 in later spots of the array follows a geometric distribution. On the other end, 9 will only ever be in the 8th or 9th index (zero-indexing), or more generally for a shuffle size of 2 a value in the nth index can only appear in the n-1 index or the nth index. Even more generally, for shuffle size of s, a value in the nth index (zero-indexing) can appear in the max(0, n-s+1) index, e.g. with a shuffle size of 10 the 10th element or 9 index can appear at position 9-10+1=0 index."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwiqEFWNqkDl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "674b418d-b4b2-436e-89e2-b66f89c1005a"
      },
      "source": [
        "ds = tf.data.Dataset.from_tensor_slices(np.arange(10))\n",
        "print([x.numpy() for x in ds])\n",
        "ds_shuffle = ds.shuffle(2, reshuffle_each_iteration=True)\n",
        "for _ in range(20):\n",
        "    print([x.numpy() for x in ds_shuffle])"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "[1, 0, 2, 3, 5, 4, 6, 8, 9, 7]\n",
            "[1, 2, 0, 3, 4, 5, 7, 6, 8, 9]\n",
            "[0, 1, 3, 2, 5, 4, 6, 8, 7, 9]\n",
            "[1, 0, 2, 3, 5, 4, 6, 7, 9, 8]\n",
            "[1, 0, 2, 4, 3, 5, 7, 6, 9, 8]\n",
            "[0, 2, 3, 1, 4, 5, 6, 7, 9, 8]\n",
            "[0, 1, 2, 4, 5, 6, 7, 8, 9, 3]\n",
            "[1, 2, 3, 4, 0, 5, 6, 8, 7, 9]\n",
            "[0, 2, 3, 4, 5, 1, 7, 8, 6, 9]\n",
            "[0, 2, 1, 3, 5, 6, 4, 8, 9, 7]\n",
            "[0, 1, 2, 3, 5, 6, 7, 4, 9, 8]\n",
            "[1, 2, 3, 0, 4, 6, 7, 5, 9, 8]\n",
            "[0, 2, 1, 3, 4, 6, 5, 7, 9, 8]\n",
            "[1, 0, 3, 4, 5, 6, 7, 2, 8, 9]\n",
            "[1, 2, 0, 4, 5, 6, 7, 3, 8, 9]\n",
            "[1, 2, 0, 3, 5, 4, 7, 6, 9, 8]\n",
            "[1, 0, 3, 2, 4, 6, 5, 8, 9, 7]\n",
            "[1, 2, 0, 3, 5, 6, 7, 4, 8, 9]\n",
            "[0, 2, 1, 4, 5, 6, 3, 7, 8, 9]\n",
            "[0, 2, 3, 1, 5, 6, 4, 7, 9, 8]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvHPeW3O-_wV",
        "colab_type": "text"
      },
      "source": [
        "Another example of shuffle, but with a tuple input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPaA5q7b_EMy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "outputId": "b8d0da82-f80e-46fa-d8c7-510f9be95e53"
      },
      "source": [
        "rand_ds = tf.data.Dataset.from_tensor_slices((np.random.randn(10, 5), np.arange(10)))\n",
        "for arr, label in rand_ds:\n",
        "  print(arr.numpy(), '\\t', label.numpy())"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-0.17038979 -1.01244879  0.17762838 -1.24809581  0.36346598] \t 0\n",
            "[ 1.48327704 -0.28145836  0.41544126 -0.54829399  0.24367764] \t 1\n",
            "[-0.37245552  0.86221593 -1.40354516  1.41368913 -0.44068334] \t 2\n",
            "[ 1.20110936 -1.47307629 -0.15802795 -0.67009457 -1.02665006] \t 3\n",
            "[ 1.2861011   0.15637034 -0.83163134  1.89373309  1.79107898] \t 4\n",
            "[ 0.09053548  0.05684375 -0.14790181 -0.85165972 -0.60498225] \t 5\n",
            "[ 0.86458806 -2.2202643   0.32900089 -1.29260993  0.85133427] \t 6\n",
            "[-1.33570526  0.21023918 -0.5957493   0.23565526  0.7347443 ] \t 7\n",
            "[ 1.01661854  0.089515   -1.06593683  0.42040595  0.18637991] \t 8\n",
            "[-1.53374496 -0.08477672  0.01286809  0.6599574   0.63874449] \t 9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L630KzEw_Z8Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(10):\n",
        "  ds_iter = iter(rand_ds.shuffle(2, reshuffle_each_iteration=True))\n",
        "  arr, label = next(ds_iter)\n",
        "  print(arr.numpy(), '\\t', label.numpy())\n",
        "  arr, label = next(ds_iter)\n",
        "  print(arr.numpy(), '\\t', label.numpy())\n",
        "  print('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pa4v-SjpnYIr",
        "colab_type": "text"
      },
      "source": [
        "# Datasets with x, y"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-XFtZiyncz1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train, test = tf.keras.datasets.fashion_mnist.load_data()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ka7zRUBW5C0m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "images, labels = train\n",
        "images = images/255.0\n",
        "labels = labels.astype(np.int32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abrtBhLU5gbX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnist_ds = dataset.shuffle(5000).batch(32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quxNWfXan6pq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnist_ds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_yww7s26L4k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "images_reshaped = images.reshape((60000, -1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gvtS2cD_gPj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "type(images_reshaped)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jL8j1V3F6Xaj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "images_reshaped.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-LzTcvY6ZXT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ds_linear = tf.data.Dataset.from_tensor_slices((images_reshaped))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAjf3Rtq6guU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnist_ds_linear = ds_linear.shuffle(10).batch(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "538UZyRu6oqj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnist_ds_linear"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3oHp9DSh6q3T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for num_now in ds_linear.take(2):\n",
        "  plt.figure(); plt.imshow(np.reshape(num_now, (28, 28)), cmap='gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1uwEbjl7QAo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for num_now in ds_linear.shuffle(5000).take(2):\n",
        "  plt.figure(); plt.imshow(np.reshape(num_now, (28, 28)), cmap='gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bvx04TsR5ZxX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fmnist_train_ds = tf.data.Dataset.from_tensor_slices((images, labels))\n",
        "fmnist_train_ds = fmnist_train_ds.shuffle(5000).batch(32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6e4SxXpmOgzw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(10)\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vV1BmXWBO_Ti",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
        "              metrics=['accuracy'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mbdR45XPARV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.fit(fmnist_train_ds, epochs=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSAORFjzPSYr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result = model.predict(fmnist_train_ds, steps = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fljyfl1BPU-D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_ds = tf.data.Dataset.from_tensor_slices((images, labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9dcQwHlRX3B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x, y = list(zip(*list(test_ds.batch(5).take(1).as_numpy_iterator())))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ff7DBvU_RZsJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nsv_NPJzQBZc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x, y = list(zip(*list(test_ds.batch(5).take(1).as_numpy_iterator())))\n",
        "y_predict_all = model.predict(test_ds.batch(5), steps=1)\n",
        "y_top = np.argmax(y_predict_all, axis=1)\n",
        "print(y, y_top)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MaCEj-jIQS01",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for image_now in x[0]:\n",
        "    plt.figure()\n",
        "    plt.imshow(image_now.squeeze(), cmap='gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUImNHuL2sGk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_lin = tf.range(0, 100, dtype=tf.float32)\n",
        "y_lin = tf.range(0, 100, dtype=tf.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUMJKSAL3A0Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_ds = tf.data.Dataset.from_tensor_slices(x_lin)\n",
        "y_ds = tf.data.Dataset.from_tensor_slices(y_lin)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgRtWDOkA-WG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def add_noise(x):\n",
        "    return tf.random.normal(x.shape, mean=x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fArSTK363LJ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_noisy = x_ds.map(add_noise)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8bJ4NTb4jOh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xy_ds = tf.data.Dataset.zip((x_noisy, y_ds))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zo-VPie33w0C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "     tf.keras.layers.Dense(10, input_shape=[1]),\n",
        "     tf.keras.layers.Dense(1)]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzpuWJbZ4Ugj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss=\"mse\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B11CCqfjAdC9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xy_ds.take(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3zUJMrf4btG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.fit(xy_ds.batch(5).repeat(), epochs=100, steps_per_epoch=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90R79fP-NEpj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.predict(xy_ds.take(1), steps=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nmlVfZXxDjc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82VJJrMKClxf",
        "colab_type": "text"
      },
      "source": [
        "# Numpy (python) wrap"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aP7_kHAcFyec",
        "colab_type": "text"
      },
      "source": [
        "Sometimes it will be easier to repurpose previously written Python code, most likely a function using numpy or scipy, than rewriting it with tensorflow methods. The dataset objects are fundamentally built for graph mode, even if there are ways to expose pieces with eager execution, and as such datasets want to operate and use tensors. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWQHpCydCoSq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def add_random_noise(x, mu=0.0, std=1.0):\n",
        "    if isinstance(x, np.float):\n",
        "        x_noisy = x + std * np.random.randn(1) + mu\n",
        "    else:\n",
        "        x_noisy = x + std * np.random.randn(*x.shape) + mu\n",
        "    return np.float32(x_noisy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a82VTaOpIPfJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "arr = np.array([7, 2, 1, 6, 3, 5, 9], dtype=np.float32)\n",
        "dataset = tf.data.Dataset.from_tensor_slices(arr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bmj3PQFLI9tc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function(input_signature=[tf.TensorSpec(None, tf.float32)]) \n",
        "def tf_add_random(input):\n",
        "    mu = 0.\n",
        "    sigma = 1.\n",
        "    np_random = lambda x: add_random_noise(x, mu, sigma)\n",
        "    y = tf.numpy_function(np_random, [input], tf.float32) \n",
        "    return y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dD5GTBSJNN3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "list(dataset.map(tf_add_random).as_numpy_iterator())"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}